{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Ensembles and Final Result\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Train an AdaBoost classifier and compare its performance to results obtained in Part II using 10 fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noshow_lib.util as utils\n",
    "import noshow_lib.preprocess as preprocess\n",
    "import numpy as np\n",
    "\n",
    "file_config = utils.file_config\n",
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 19.5min finished\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost code goes here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier(base_estimator=rf)\n",
    "scores_auc = cross_val_score(abc, train_X, train_y, cv=10, scoring='roc_auc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688813816530363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score for the original random forest classifier was 0.6922, so the adaboost is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgBoost\n",
    "\n",
    "Train an xgBoost classifier and compare its performance to results in Part II using 10 fold CV. `sklearn` has a gradient boosting model included http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html which you can use. The `xgboost` package https://xgboost.readthedocs.io/en/latest/python/python_intro.htmlhas a wrapper you can use with sklearn as well https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn. The latter is more efficient at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "# xgboost code here\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = xgb.XGBRegressor()\n",
    "scores_auc = cross_val_score(gbc, train_X, train_y, cv=10, scoring='roc_auc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73193051389827546"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xgBoost did better than all other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Choose a set of 5 or so classifiers. Write a function that trains an ensemble using stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_stack_ensemble(X, y):\n",
    "    # create train/validation sets\n",
    "    # using StratifiedShuffleSplit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
    "\n",
    "    # train classifiers in ensemble using train set\n",
    "    c1 = xgb.XGBRegressor().fit(X_train,y_train)\n",
    "    c2 = LinearSVC().fit(X_train,y_train)\n",
    "    c3 = RandomForestClassifier(n_estimators=100).fit(X_train,y_train)\n",
    "    c4 = RandomForestClassifier(n_estimators=100,criterion='entropy').fit(X_train,y_train)\n",
    "    c5 = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "    \n",
    "    # create new feature matrix for validation\n",
    "    # set by getting predictions from the ensemble\n",
    "    # classifiers\n",
    "    X_predict = np.stack((c1.predict(X_test),c2.predict(X_test),c3.predict(X_test),c4.predict(X_test),c5.predict(X_test))).T\n",
    "    \n",
    "    # train logistic regression classifier on\n",
    "    # new feature matrix\n",
    "    c = LogisticRegression().fit(X_predict,y_test)\n",
    "    \n",
    "    # return all trained classifiers\n",
    "    return c,(c1,c2,c3,c4,c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "c,c1 = build_stack_ensemble(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold cross validation to measure performance of your stacked classifier. See Part II solution to see how to roll your own sklearn classifier along with http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class TheBestEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        self.c_, self.cs_ = build_stack_ensemble(X,y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_predict = np.stack((self.cs_[0].predict(X),self.cs_[1].predict(X),self.cs_[2].predict(X),\n",
    "                              self.cs_[3].predict(X),self.cs_[4].predict(X))).T\n",
    "        return self.c_.predict(X_predict)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_predict = np.stack((self.cs_[0].predict(X),self.cs_[1].predict(X),self.cs_[2].predict(X),\n",
    "                  self.cs_[3].predict(X),self.cs_[4].predict(X))).T\n",
    "        return self.c_.predict_proba(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbe = TheBestEnsemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73809124  0.73509764  0.73755287  0.74551324  0.73276404  0.73774204\n",
      "  0.73577124  0.73673849  0.74308787  0.72983612]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  7.0min finished\n"
     ]
    }
   ],
   "source": [
    "scores_auc = cross_val_score(tbe, train_X, train_y, cv=10, scoring='roc_auc', verbose=1)\n",
    "print scores_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737219479022\n"
     ]
    }
   ],
   "source": [
    "print np.mean(scores_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only slightly better than the xgBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Choose a single model based on all previous project steps. Train this model on the complete training dataset and measure it's performance on the held out test set.\n",
    "\n",
    "Compare to the 10-fold CV estimate you got previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My ensemble gave the best results so I will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheBestEnsemble()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final result goes here\n",
    "tbe = TheBestEnsemble()\n",
    "tbe.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_y = preprocess.load_test_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740357504247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "fscore = roc_auc_score(test_y,tbe.predict_proba(test_X)[:,1])\n",
    "print fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Actually did better than the cv results with the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
